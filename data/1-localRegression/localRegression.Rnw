%%%This Is a document preamble which is intended to be precompiled with
%%%
%%%  pdflatex -ini -jobname="<document base name>" "&pdflatex preamble\dump"
%%%
%%%  Last edited by Cameron Bracken on 2008 Oct 10

\documentclass[11pt,twoside]{article}

%%------------font choices
\usepackage[sc]{mathpazo}
\linespread{1.05}         % Palatino needs more leading (space between lines)

%Sweave
<<sweave,echo=F,results=hide,fig=F>>=
if(!file.exists('Sweave.sty'))
	file.copy(file.path(R.home(),'share','texmf','Sweave.sty'),'.')
@
\usepackage[noae,nogin]{Sweave}

%%------------PGF/TIKZ
\usepackage{tikz}

%%------------page layout
\usepackage[left=2cm,top=3cm,right=2cm,bottom=2cm]{geometry} %changes margins
\usepackage[parfill]{parskip} % begin paragraphs with an empty line not indent
\usepackage{multicol}

%%-----------section styles
\usepackage{sectsty}
	%Put period after section number
\sectionfont{\bf\large\raggedright}
\subsectionfont{\bf\normalsize\raggedright}
\subsubsectionfont{\bf}

%%------------graphics
\usepackage{graphicx} 
\usepackage{subfigure}

%%------------mathematics
%\usepackage{amsmath,amssymb,amsthm}

%%------------tables
\usepackage{booktabs}

%%------------misc
\usepackage{verbatim} 
\usepackage[pdftex,bookmarks,colorlinks,breaklinks]{hyperref}
\hypersetup{linkcolor=black,citecolor=black,filecolor=black,urlcolor=black}

%%------------bibliography
\usepackage{natbib}   
%\setcitestyle{square,aysep={},yysep={;}}
\bibliographystyle{agufull04}

%%-----------nicer looking captions
\usepackage[font={bf,small},textfont=md,margin=30pt,aboveskip=0pt,belowskip=0pt]{caption}

%%-----------page header declaration
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\fancyhead[LE,RO]{\thepage}   %page numbers

\fancyhead[CE]{\small CVENxxxx FALL 2009}
\fancyhead[CO]{\small LOCAL POLYNOMIAL REGRESSION}

%\long\def\beginpgfgraphicnamed#1#2\endpgfgraphicnamed{\includegraphics{#1}}
\pgfrealjobname{localRegression} 

\begin{document}

	%Sweave options
\SweaveOpts{echo=F, prefix.string=figs/fig, fig=T, pdf=F, eps=F, pgf=F, tikz=F, external=T}
\thispagestyle{empty} 

\begin{center}
	\textbf{\Large Local Polynomial Regression}
 	{\bf\\ Cameron Bracken \\}
  	CVEN6xxx September, 2009
\end{center}

<<setup,fig=F,echo=F,results=hide,cache=F>>=
	require(locfit)
	require(blocfit)
	require(xtable)	
	require(tikzDevice)
	require(pgfSweave)
	
	cashdir = "./cache"
	setCacheDir(cashdir)
	
	if(!file.exists('figs')) dir.create('figs')
	
	options('tikzDocumentDeclaration' = '\\documentclass[11pt]{article}')

@

\section*{Problem 1}

The Local polynomial regression was implemented in a package dubbed \texttt{blocfit}. The functions behave exactly like the LOCFIT package.  The \texttt{blocfit()} function initializes the fit object, the \texttt{predict.blocfit()} function estimates the function at arbitrary points by calling \texttt{predict.point()}.

\begin{Verbatim}[frame=single]
blocfit <- 
function(x, y, a = 0.7, p = 1, kern = 'bisq'){
	
	obj <- list(x = x, 
			y = y, 
			a = a, 
			n = length( y ), 
			nn = round( a*length( y ) ), 
			p = p, 
			W = chooseWeightFunction( kern )	)
	
	pred <- predict.blocfit( obj, Lmat = TRUE)
	
		# degrees of freedom 
	obj$v1 <- sum(diag(pred$L))
	obj$v2 <- sum(diag(t(pred$L) %*% pred$L))
	
	obj$fittedValues <- pred$fittedValues
	obj$resid <- x - pred$fittedValues
	obj$sigma <- sd( obj$resid )
	obj$c <- qnorm(.95)
	obj$gcv <- obj$n * sum(obj$resid^2) / ( obj$n - obj$v1 )^2
	
	class( obj ) <- "blocfit"
	return( obj )

}
\end{Verbatim}

\begin{Verbatim}[frame=single]
predict.blocfit <- function(fit, at = fit$x, Lmat = FALSE){
	
	np <- length(at)
	fittedValues <- numeric(np)
	L <- if(Lmat) matrix(NA, nrow = np, ncol = fit$n) else NULL
	
		#for all the points to predict get the fitted values
	for(i in 1:np){
		
		pred <- predict.point(fit, at[i])
		fittedValues[i] <- pred$yhat
		if(Lmat) L[i,] <- pred$li
		
	}
	
	return( list(fittedValues = fittedValues, L = L) )
	
}
\end{Verbatim}

\begin{Verbatim}[frame=single]
predict.point <- function(obj, at, dist = 'global'){

		# create the "design" matrix, from taylor expansion
	x <- obj$x - at
	X <- if(obj$p == 1){
			cbind(rep(1,obj$n), x)
		}else if(obj$p == 2){
			cbind(rep(1,obj$n), x, x^2)
		}else
			cbind(rep(1,obj$n), x, x^2, x^3)
		
			
		# Distance from estimate to other points
	d <- as.matrix( dist( scale( c(at, obj$x) ) ) )[1,2:(obj$n+1)]
		
		#scale distances to [0,1]
	dk <- d[order(d)[obj$nn]]
	d[d > dk] <- dk
	d <- d/dk
	
		#calculate weights
	W <- diag(obj$W(d))
	
		# get influence vector
	e1 <- c(1,rep(0,obj$p))
	li <- e1 %*% ( solve(t(X) %*% W %*% X) ) %*% ( t(X) %*% W )

	yhat <- sum(li * obj$y)
	
	o <- list(at = at, yhat = yhat, li = li)
	return(o)
	
}
\end{Verbatim}

The benefit of the object oriented design is usability.  After the methods have been written, customizing the fit is extremely easy.  Plotting is also dine with a method \texttt{plot.blocfit()}:

\begin{Verbatim}[frame=single]
plot.blocfit <- 
function(obj, get.data = TRUE, abline = TRUE, 
	get.ci = TRUE, legend = TRUE, percent = "%"){
	
	if(class(obj) != "blocfit") stop('object is not of class blocfit')
	
	xnew <- seq(range(obj$x)[1],range(obj$x)[2],length.out=100)
	pred <- predict.blocfit( obj, at = xnew )
	ci <- ci.blocfit( obj, xnew )
	
		# plot the fit 
	plot(obj$x,obj$y,type='n', xlab = "X", ylab = "Y")
	
		#upper and lower confidence intervals
	if(get.ci) lines(xnew,ci[1,],lty='dashed')
	if(get.ci) lines(xnew,ci[2,],lty='dashed')
	
		#plot the data points
	if(get.data) points(obj$x,obj$y)
	
		#plot the linear regression line
	if(abline) abline(lm(obj$y~obj$x),col='red')
	
	lines(xnew,pred$fittedValues)
	
		#plot a legend
	if(legend){
		ltext <- c("Fit")
		if(get.data) ltext <- c(ltext, "Data")
		if(get.ci) ltext <- c(ltext, paste("95",percent,"CI"))
		if(abline) ltext <- c(ltext, "Linear Reg")
		
		legend("topleft", 
				ltext, 
				lty = c(1,0,2,1),
				pch = c(-1,1,-1,-1),
				col = c(1,1,1,2))
	}
	
}
\end{Verbatim}

\begin{figure}[!ht]
\centering
<<deg1, echo=T, fig=T, tikz=T, cache=T,width=5,height=5,keep.source=T>>=

	#Data object from the blocfit package containing test data
data(oneD)

x <- oneD$x; y <- oneD$y

blfit <- blocfit(x, y, a=.5, p=1, kern='bisq')
plot(blfit, percent = "\\%")
	
@
\caption{Sample data with local polynomial fit, $p=\Sexpr{blfit$p}$, $\alpha=\Sexpr{blfit$a}$}\label{deg1}
\end{figure}



\begin{figure}[!ht]
\centering
<<linear, echo=T, fig=T, tikz=T, cache=T,width=5,height=5>>=

	blfit <- blocfit(x, y, a=1, p=1, kern='none')
	plot(blfit, percent = "\\%" )	
	
	
@
\caption{Sample data with local polynomial fit, $p=\Sexpr{blfit$p}$, $\alpha=\Sexpr{blfit$a}$ and uniform weight structure.}\label{linear}
\end{figure}


\clearpage
\section*{Problem 2}

First read in the data:
<<readData, fig=F, echo=T, cache=T>>=
	data <- read.table('colo_precip.dat',header=TRUE)
	y <- data$precip
	xdata <- as.data.frame(data[c('lat','lon','elev')])
	x <- as.matrix(xdata)
	
	dem <- as.matrix(read.table('colo_dem.dat'))
	demx <- dem[,1]
	demy <- dem[,2]
	demz <- dem[,3]
@

Then find the best model and fit it:
<<locfit-calcs, fig=F, cache=T, echo=T, keep.source=T>>=

n <- length(y)
a <- seq(0.2,1.0,by=0.05)

	# get the gcv values for all combinations of deg and alpha
d1 <- gcvplot(y~x, deg=1, alpha=a, kern='bisq', scale=T, ev=dat())
d2 <- gcvplot(y~x, deg=2, alpha=a, kern='bisq', scale=T, ev=dat())
	
gcvs <- c(d1$values,d2$values)
best <- order(gcvs)[1]
	#get the best alpha and degree
bestd <- c(rep(1,n),rep(2,n))[best]
bestalpha <- c(a,a)[best]

fit <- locfit(y~x, deg=bestd, alpha = bestalpha, kern='bisq', scale=T)
pred <- predict( fit, newdata = dem, se.fit=T )

lmfit <- lm(y~x)
lmpred <- predict( lmfit, data.frame(x=dem))

print(lmfit)
print(lmpred)

@

Then plot the results:
\begin{figure}[!ht]
\centering
<<image-locfit, fig=T, tikz=T, external=T, echo=T,cache=T>>=

nbcol <- 20
nx <- length(unique(demx))
ny <- length(unique(demy))
locfitgrid <- matrix(pred$fit, nrow = ny, byrow=T)
lmgrid <- matrix(pred$fit, nrow = ny, byrow=T)
demx <- sort(unique(demx))
demy <- sort(unique(demy))

image(demy,demx,locfitgrid,col=topo.colors(nbcol),xlab='Longitude',ylab='Latitude')
contour(demy,demx,locfitgrid,add=T)

@
\caption{}\label{lfimage}
\end{figure}

\begin{figure}[!ht]
\centering
<<image-lm, fig=T, tikz=T, external=T, echo=T,cache=T>>=
image(demy,demx,lmgrid,col=topo.colors(nbcol),xlab='Longitude',ylab='Latitude')
contour(demy,demx,lmgrid,add=T)
@
\caption{}\label{lfimage}
\end{figure}

\begin{figure}[!ht]
\centering
<<persp-locfit, fig=T, tikz=T, external=T, echo=T, cache=T>>=

	colf <- function(z){
		nrz <- nrow(z)
		ncz <- ncol(z)
		# Generate the desired number of colors from this palette
		# Compute the z-value at the facet centres
		zfacet <- z[-1, -1] + z[-1, -ncz] + z[-nrz, -1] + z[-nrz, -ncz]
		# Recode facet z-values into color indices
		facetcol <- cut(zfacet, nbcol)
		facetcol
	}
	color <- topo.colors(nbcol)
	
	persp(demy,demx,locfitgrid,col=color[colf(locfitgrid)],xlab='Longitude',ylab='Latitude', zlab = 'Rainfall',
		theta=30,phi=30,expand=0.5,shade=0.5,ltheta=-30)
		
@
\caption{}\label{lfimage}
\end{figure}

\begin{figure}[!ht]
\centering
<<persp-lm, fig=T, tikz=T, external=T, echo=T>>=
	persp(demy,demx,lmgrid,col=color[colf(lmgrid)],xlab='Longitude',ylab='Latitude', zlab = 'Rainfall',
		theta=30,phi=30,expand=0.5,shade=0.5,ltheta=-30)
@
\caption{}\label{lfimage}
\end{figure}


\clearpage
\nocite{Loader1999}
\bibliography{references}

\end{document}






